Gradient Descent  
An algorithm to find the minimum value of a function.

Given a function f(x), we want to find x such that f(x) is minimized.
This is a local optimization algorithm, meaning the solution depends on the starting x.
In general, x is a vector in n-dimensional space.

Simple example: f(x) = x^2  
1. Initialize: Pick a random x. Let’s say x_0 = 2.

2. Evaluate gradient:  
    Compute f(x_0):
        f(2) = 4, so f(x_0) = 4.
    Compute the gradient (derivative):  
        d/dx f(x) = 2x = 2(2) = 4
    Since the derivative gives the slope of positive change, 
    we actually want to move in the opposite direction:  
        x_1 = x_0 - ∇ f(x_0)

3. Apply the learning rate η:
    Let η = 0.3.
    Update step:
    x_1 = x_0 - η ∇ f(x_0)
    x_1 = 2 - (0.3 * 4) = 0.8

4. Repeat (goto 2):  
    f(0.8) = 0.64, getting closer to zero!
    Let's go again:
    x_2 = 0.8 - (0.3 * 2(0.8)) = 0.32
    f(0.32) = 0.1024, and so on.

Since we approach zero (our goal) but never actually reach it, we can stop based on:
    Fixed iterations: Run for a set number of steps.
    Threshold ε: Stop when f(x) is sufficiently small.
    Gradient threshold: Stop when ∇ f(x) ≈ 0.

Python’s `autograd` package can compute derivatives automatically:
```
from autograd import grad
```

In machine learning, gradient descent minimizes the loss function (error), 
improving model predictions over time.