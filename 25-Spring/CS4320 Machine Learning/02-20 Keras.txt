Keras vs. Other Frameworks
    Keras is a high-level framework (often used with TensorFlow) that simplifies the process of building neural networks.  
    You could also use TensorFlow or PyTorch directly, but Keras typically provides a more user-friendly API.

Data Preprocessing Strategy
    scikit-learn generally offers more convenient tools for data preprocessing than Keras/TensorFlow (ex:, 'PolynomialFeatures', 'SimpleImputer', scaling, etc.).
    Plan: 
        1. Build a scikit-learn pipeline that handles all preprocessing (ex:, polynomial features, imputation, scaling).  
        2. At the end of this pipeline, do not attach a model; just transform the data.  
        3. Fit the pipeline on the training data, then transform both the training and test sets so you have “cleaned” numerical arrays ready for Keras.  
        4. Keep your ID and label/target out of the pipeline. They do not need the same transformations as your feature columns. You’ll add them back in or keep them separate as needed.

Building the Keras Model
    1. Input Layer: 
        Must match the shape (number of features) of your preprocessed data. 
        One neuron per input feature.  
    2. Hidden Layers: 
        You can experiment with different numbers of layers and different widths (neurons per layer).  
        Example: 'Dense(64, activation='relu')' repeated several times.  
    3. Output Layer:
        Depends on the problem. For binary classification, a single output neuron with a sigmoid activation.  
        For multi-class classification, you’d have one neuron per class with a softmax activation.  
    4. Compile:
        Optimizer: ex: 'tf.keras.optimizers.SGD(learning_rate=0.1)' or 'tf.keras.optimizers.Adam()'.  
        Loss function: ex: 'binary_crossentropy' for binary classification.  
        Metrics: ex: 'AUC', 'accuracy', etc. (Doesn’t guide training but helps you monitor performance.)  
    5. Training:
        Choose the number of epochs (full passes over the training data).  
        With SGD (Stochastic Gradient Descent), data is shuffled each epoch and processed in “batches.”  
        Consider using callbacks for early stopping and model checkpointing (ex:, stop when validation metric no longer improves).

Tuning Tips
    If you’re underfitting (model performs poorly on both training and validation sets):
        Increase model capacity (more layers or more neurons per layer).  
        Reduce regularization.  
        Adjust learning rate.  
    If you’re overfitting (model performs well on training but poorly on validation):
        Reduce model capacity (fewer/smaller layers) or add regularization (dropout, weight decay).  
        Use early stopping or gather more data.  
        Lower the learning rate if needed.  

Experiment with different optimizers, learning rates, and architectures—like wearing “stretchy pants,” you can adjust the model shape to fit the data better.