Thus far, we've been dealing with supervised learning and regression.
    Supervised Learning
        The model is trained using labeled data (we provide both the input and the expected output).
    Regression
        The model predicts continuous numerical values.
        It finds the best-fit line or curve.
        Predictive: Given new data, the model estimates a numerical outcome.
        In scikit-learn, these models are called "estimators".

Now we're moving onto classification.
It's still supervised, but we're predicting discrete categories rather than continuous values.
Instead of "given an X, predict a Y," we're asking, "Given an X and a Y, what 'class' (group) does this belong to?"
We're sorting, not estimating.
Some algorithms return True or False, while others return a probability value between 0 and 1.

Different Types of Classifiers
    1. Binary Classifier
        Distinguishes between two different classes.
    2. Multi-Class Classifier
        Distinguishes between more than two different classes.
        Requires different algorithms.
    3. One-vs-All (OvA) or One-vs-Rest (OvR)
        Some algorithms work well for binary classification but not for multi-class.
        A trick: Classify each class separately as "A or not A," "B or not B," and "C or not C." This effectively creates a multi-class classifier.
        In some cases, this approach is better than building a native multi-class classification algorithm.
        Uses N classifiers, where N = number of classes.
    4. One-vs-One (OVO)
        Instead of N classifiers, OVO builds N(N-1)/2 classifiers.
        Each classifier is trained on a pair of classes.
        Can be computationally expensive for large class counts.

Some classification algorithms:
    Decision Trees – Recursively choose features that provide the best "split" and create a corresponding tree; leaf nodes provide probabilities.
    Logistic Regression – Predicts which side of the decision boundary an input belongs to.
    Nearest Neighbors – Computes the Euclidean distance to the nearest samples in the training data and classifies based on the majority.
    Support Vector Classification (SVC) – Maximizes the width of the "road" (margin) between classes.

Metrics (how we measure the quality of a model)
Accuracy
    The raw percentage of correct predictions.
    Limitation: It ignores class imbalances (e.g., 95% overall accuracy but only 60% accuracy for Group B).