How do we handle missing data points?
- Dropping rows with missing data (.dropna())  
  - If every row has *some* missing value, dropping the row entirely is not practical.  
  - Instead, you can selectively drop rows on a *per-feature* basis. For example:  
    ```
    # Select just the columns you want, then drop missing data in those columns
    tmpdf = df[["x", "y"]].dropna()
    ```
  - Typically, you only drop rows for specific tasks like generating a plot or computing a metric for those columns. You still keep the full dataset for other uses (e.g., training).

What about categorical (not numerical) data?
- Why we don’t just assign numbers to categories  
  - Assigning arbitrary numeric codes (e.g., A=0, B=1, C=2) implies a numerical relationship (like “C > B”) which doesn’t make sense for nominal categories.  
  - Our model tries to interpret coefficients in a linear (or numerical) space, so the model would see A < B < C. That’s often incorrect for purely categorical data.

- One-Hot Encoding  
  - For a categorical feature with possible values \[A, B, C\], we create separate binary columns—one for each category.  
  - Example:  
    ```
    BldgType: 
      1Fam, 2FmCon, Duplx, TwnhsE, TwnhsI
    ```
    Instead of encoding as 1=1Fam, 2=2FmCon, etc., we split it into five columns:
    ```
    BldgType_1Fam   BldgType_2FmCon   BldgType_Duplx  BldgType_TwnhsE   BldgType_TwnhsI
          1               0                 0               0                0
          0               1                 0               0                0
          ...
    ```
  - Each of those columns (e.g., BldgType_1Fam) can be 0 or 1, and the model then learns separate coefficients for each column.

- Handling unknown categories  
  - In validation or production, sometimes you’ll encounter categories that were never seen during training.  
  - Many encoders (e.g., sklearn’s OneHotEncoder) allow a parameter like handle_unknown='ignore' to skip unseen categories without causing errors.

3. Combining Numerical and Categorical Features

- Typically, your preprocessing pipeline will have two parts:  
  1. Numerical Preprocessing (e.g., scaling, imputation for missing numeric values)  
  2. Categorical Preprocessing (e.g., one-hot encoding)  
- You then merge or concatenate these transformed features (sometimes via FeatureUnion in scikit-learn) into a single feature set that you feed into the model.

Example using scikit-learn pipelines and feature unions might look like:
```
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

numeric_features = ["num_col1", "num_col2"]
numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="mean")),
    ("scaler", StandardScaler())
])

categorical_features = ["cat_col1", "cat_col2"]
categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, numeric_features),
        ("cat", categorical_transformer, categorical_features)
    ]
)

# Then integrate the preprocessor into a full pipeline
# with your estimator (model) of choice
```