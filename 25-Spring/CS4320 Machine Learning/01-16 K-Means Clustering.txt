K-Means Clustering
An unsupervised machine learning algorithm used to group data points into clusters 
based on similarity.
Commonly applied in scenarios such as:
- Market segmentation
- Anomaly detection (ex: fraud detection)
- Image segmentation
- Social network analysis
Focuses on distinct groups by dividing data points into non-overlapping clusters.
Uses a similarity score (e.g., cosine similarity, Euclidean distance).
Minimizes intra-cluster distances (points within a cluster) and 
maximizes inter-cluster distances (points between clusters).


K-Means Clustering Steps
1. Initialize k:
   - k = number of clusters.
   - Randomly pick k initial centroid points.
2. Distance Calculation:
   - Compute the Euclidean distance of each data point from all centroid points.
3. Assign Data Points:
   - Assign each data point to the nearest centroid, forming initial clusters.
4. Recompute Centroids:
   - Calculate the average position of all points in a cluster to update the centroid.
5. Iterate Until Convergence:
   - Repeat steps 2-4 until centroids stop moving (convergence).


Choosing the Right k
1. Elbow Method:
   - Uses Within-Cluster Sum of Squares (WCSS), which measures the variance within clusters.
   - Plot k vs. WCSS:
     - Look for the "elbow" where WCSS drops dramatically, indicating the optimal k.
2. Silhouette Method:
   - Measures how well data points are clustered using Silhouette Coefficient (SC):
     SC = (b_i - a_i) / max(a_i, b_i)
     - a_i: Average distance to points within the same cluster (intra-cluster).
     - b_i: Average distance to points in the nearest different cluster (inter-cluster).
   - Score ranges:
     - 1: Perfectly clustered; dense clusters far apart.
     - 0: Clusters are indifferent or overlapping.
     - -1: Poor clustering; points are misclassified.
   - Calculate the average SC for different k values; highest SC indicates optimal k.


Normalize data (Min-Max scaling) to ensure features with large magnitudes 
(e.g., income vs. age) do not dominate the distance calculation.


Advantages
- Simple to understand and apply.
- Guarantees convergence.
- Efficient for large datasets.
Drawbacks
- Requires specifying k upfront.
- Sensitive to initial centroids (may converge to suboptimal solutions).
- Struggles with outliers or irregularly shaped clusters.


* Orange: A user-friendly ML tool to experiment with clustering and visualization.