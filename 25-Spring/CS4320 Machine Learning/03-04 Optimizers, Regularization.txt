Optimizers
The goal of an optimizer is to minimize the loss more quickly (in fewer epochs) while still reaching an optimal solution

"Stochastic Gradient Descent" (SGD) is an iterative method for optimizing an objective function. It can be regarded as a stochastic approximation of gradient descent optimization, since it replaces the actual gradient (calculated from the entire data set) by an estimate thereof (calculated from a randomly selected subset of the data). Especially in high-dimensional optimization problems this reduces the very high computational burden, achieving faster iterations in exchange for a lower convergence rate.

Some extensions and variants of SGD:

   One example is using a learning rate schedule in gradient descent to reduce eta η (learning rate or "step size") over time

   Using a momentum value β (for example, momentum = 0.9) adds a fraction of the previous update to the current update, which helps overcome local minima or flat regions

   Nesterov Accelerated Gradient looks ahead before updating weights, can give faster convergence compared to standard momentum. (nesterov = True)

   Adagrad (adaptive gradient algorithm): scales the gradient based upon the gradient size in different directions. 

   RMSProp (Root Mean Square Propagation): the idea is to divide the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight. (RMSprop(rho=0.9))

   Adam (Adaptive Moment Estimation): adapts the learning rate for each parameter based on past gradients. An update to RMSProp and a popular default choice for many tasks. (Adam(beta_1=0.9, beta_2=0.999))

SGD with momentum is great, RMSProp is even better, Adam is fantastic and usually the go-to default.

* * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * *

Regularization - techniques we use to try to prevent overfitting. In other words, regularization improves generalization.

if we let coefficients of features or terms shrink to zero when they're found to be unimportant, we can avoid overfitting to noise in the training data.
L1 (Lasso) - minimize the absolute value
L2 (ridge regression) - minimize the square

Early stopping - Stops training when validation performance deteriorates, preventing overfitting by halting before the model memorizes training data

Dropout - repeatedly ignores random subsets of neurons during training, which simulates the training of multiple neural network architectures at once to improve generalization. each layer is given a parameter, say 50% dropout rate, and during any particular forward pass, half of the neurons will be randomly shut off.