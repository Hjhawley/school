ML Project Outline

1. Look at the Big Picture
    Clearly define the problem you are trying to solve.
    Identify the type of machine learning task: regression, classification, clustering, etc.
    Understand your goals, success metrics (accuracy, RMSE), and any constraints (time, compute, budget).

2. Gather the Data
    Collect a dataset that is relevant, complete, and representative of the problem domain.
    Document the data sources and make sure they are reliable.
    If necessary, combine multiple data sources (data fusion) and check for consistency.

3. Explore and Visualize to Gain Insight
    Use summary statistics and visualizations (histograms, scatter plots, pair plots, heatmaps) to understand:
        Data distributions
        Correlations between variables
        Potential patterns or trends
        Outliers and anomalies
    Ask exploratory questions: 
        Which features are most relevant? 
        Are there redundancies or unnecessary data?

4. Prepare Data for ML Algorithms (Preprocessing)
    Clean the Data:
        Handle missing data:
            Remove data points entirely: Use sparingly, as it may reduce dataset representativeness.
            Impute missing data: Options include:
                Randomly select a value (not recommended unless the missing data is random and minimal).
                Use mean: Works for symmetric distributions.
                Use median: More robust for skewed distributions.
                Advanced techniques: Predict missing values using other features (k-Nearest Neighbors, regression).
        Correct data errors or inconsistencies.
        Remove or adjust outliers as needed (but understand their potential significance first).

    Feature Engineering:
        Add new features if useful.
        Remove irrelevant or redundant features.

    Scale and Normalize Data:
        Normalize or standardize features to make sure consistent ranges and magnitudes:
            x' = (x - mean) / standard deviation
            Standardization (also called Z-score normalization) rescales the data so that it has:
                A mean of 0
                A standard deviation of 1

    Split the Data:
        Split into training (80%) and testing (20%) datasets.
        USE A KNOWN SEED for reproducable results

5. Select a Model and Train It
    Choose an initial model based on the problem type (regression for continuous data, decision trees for categorical predictions).
    Train the model on the training dataset.
    Evaluate its performance using metrics suited to the task (MAE, MSE, accuracy, F1 score).

6. Fine-Tune the Model
    Optimize hyperparameters using techniques like grid search or random search.
    Experiment with adding/removing features or adjusting model complexity.
    Use cross-validation to make sure generalizability.
    Perform feature scaling (if not done already) to improve optimization and model convergence.
    Regularization (L1/L2) may be added to prevent overfitting.

7. Test the Model
    Evaluate the trained and tuned model on the testing dataset (data it has never seen before).
    Compare performance on the testing set against your chosen metrics to assess how well the model generalizes.
        Training loop: Train -> Model -> Test; Repeat. 
        This is still INDIRECTLY training it with the testing data, 
        so it doesn't actually tell us how well the model generalizes. "Overfitting"
        Commonly accepted solution: 
            take our dataset, 
            do the 80/20 split, 
            put testing data in a vault (leave it alone,)
            take training data and re-split it; 
                create the actual training set and the "validation" set or "dev" set
                split training set into 3 different groups x1 x2 x3:
                    use x1 as the validation set and x2+x3 as the training set 
                    use x2 as the validation set and x1+x3 as the training set
                    use x3 as the validation set and x1+x2 as the training set
                    and take the average (and maybe std dev)
                this is called "cross validation"
            keep trying different models and different variations of models until we find a good fit
            you will find plenty of ways "not to make a lightbulb" but might find something useful eventually!

8. Present Your Solution
    Clearly communicate the findings:
        Include key insights from the data exploration phase.
        Show model results using visualizations (confusion matrices, prediction plots, residuals).
        Highlight trade-offs and limitations.
    make sure the explanation is understandable to both technical and non-technical stakeholders.

9. Launch, Monitor, and Maintain Your System
    Deploy the model in production.
    Set up monitoring systems to track performance (accuracy drift, model bias, system latency).
    Update and retrain the model periodically with fresh data.
    Monitor for concept drift (i.e., changes in data patterns over time).