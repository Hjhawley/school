Quest for the Optimal Classifier

Objective: classify data into two or more groups based on certain features. 
Mathematical tools help us achieve the best possible classification.

Example: Classifying mice as "obese" or "not obese" based on mass
 - Not Obese: 11g, 12g, 14g
 - Obese: 18g, 19g, 21g
Plot the data points on a number line.

What's the optimal threshold?
1. Midpoint of max(Not Obese) and min(Obese):
(14g + 18g)/2 = 16g
2. Midpoint of mean(Not Obese) and mean(Obese):
(12.33g + 19.33g)/2 = about 15.83g
3. Something else?

Classification in one dimension (1D)
Margin: The distance between the threshold and the nearest data point from each class.
 - Goal: Maximize the margin while keeping the threshold between the classes.
 - Example: Threshold at 16g gives a margin of 2g.

What do we do about outliers?
 - Example: A not-obese mouse at 17g.
 - Do we adjust the threshold? NO
 - Outliers might be sacrificed for a better overall threshold.
 - Not every data point is equally important.

Classification in two dimensions (2D)
This time we classify obese mice by mass AND height.
 - Obese mice have large mass PROPORTIONAL to their height.
 - Massive mice that are also tall may not necessarily be obese.
 - Once again, we want to maximize the margin while maintaining a threshold between classes.
 - We draw two parallel lines ("gutters") representing the edges of each class.
 - The margin line is centered midway between these two gutters.

Margin and Hyperplanes in higher dimensions
Hyperplane: A straight line (in 2D) or a plane (in 3D) that separates two classes.
Margin: The perpendicular distance from the hyperplane to the support points (closest points from each class).
Positive and Negative Gutters: Lines parallel to the hyperplane that touch the support points.

Support Vector Classifier (SVC)
 - Maximal Margin: The largest possible margin between classes.
 - Hyperplane: The decision boundary defined by the maximal margin.
 - Support Points (Support Vectors in high dimensions): Points closest to the hyperplane that determine the margin.

Finding the Maximal Margin mathematically

w is a vector that represents the weights (or coefficients) of the hyperplane.
Each element of w corresponds to the importance of a feature in the classification decision.
Example: If you're classifying mice by mass and height, w might look like [w_mass, w_height].

Hyperplane:
    H_0: {w} * {x} + b = 0 
Parallel Gutters:
    H_1: {w} * {x} + b = k 
    H_2: {w} * {x} + b = -k 
Key Formula:
    {w} * {x} = ||{w}|| * (({||{x}|| * cos(Î¸)}) / C) = -b 