Activation Functions

What is an activation function?  
- In a neuron, an activation function takes the weighted sum of inputs and applies a non-linear (or sometimes linear) transformation. The resulting output is then passed to the next layer.  
- The choice of activation function can greatly affect how well gradient descent converges, and it influences whether gradients vanish or explode.

Common Activation Functions

1. Linear (Identity)  
   - Simply passes values straight through: output = x.  
   - It is smooth and fast, but it is generally not used in hidden layers if the goal is to learn non-linear patterns; it is often reserved for output layers in regression tasks.

2. ReLU (Rectified Linear Unit)  
   - Defined as: ReLU(x) = max(0, x).  
   - For negative x, the output is 0; for positive x, the output is x.  
   - It is fast to compute but not differentiable at x = 0; neurons can “die” if they consistently receive negative inputs.

3. Leaky ReLU  
   - Defined as: LeakyReLU(x) = max(alpha * x, x), where alpha is a small constant (for example, 0.01).  
   - It allows a small, non-zero gradient when the input is negative, which helps prevent “dead” neurons.  
   - It is slightly more computationally expensive than ReLU but remains relatively fast.

4. ELU (Exponential Linear Unit)  
   - Defined as: ELU(x) = x if x > 0; otherwise, ELU(x) = alpha * (exp(x) - 1).  
   - This function is smooth and can help gradient-based learning converge faster. It is somewhat more expensive to compute than ReLU or Leaky ReLU.  
   - A variant, called SELU (Scaled ELU), includes a scaling constant to automatically normalize outputs and is typically used in “self-normalizing” networks that use only Dense layers without Batch Normalization.

5. GELU (Gaussian Error Linear Unit)  
   - This smooth function uses the Gaussian cumulative distribution function to transform its input.  
   - While it can yield good results, it is more computationally expensive than ReLU or Leaky ReLU.

6. SiLU (Swish) / Mish  
   - These are newer smooth activations. For SiLU (also known as Swish), the function is defined as: silu(x) = x * sigma(x), where sigma(x) represents the sigmoid of x.  
   - Mish is another smooth function that can sometimes outperform other activations, though it requires more computation.

Key Takeaway:  
- Each activation function has its own advantages and disadvantages in terms of smoothness, speed, and how they affect gradient behavior.  
- ReLU and its variants remain popular due to their simplicity and good performance, but it is worth experimenting with alternatives if issues like dead neurons or slow convergence arise.

Weight Initialization

Proper weight initialization is crucial to help avoid vanishing or exploding gradients.

- RandomUniform: Samples weights from a uniform distribution. It is a general default but may lead to suboptimal convergence in deeper networks.  
- RandomNormal: Samples weights from a Gaussian distribution with a specified mean and standard deviation.  
- Glorot (Xavier) Initialization:  
  - Commonly used for layers with sigmoid, tanh, or linear activations.  
  - Can be implemented in two ways: GlorotUniform or GlorotNormal.  
  - It balances the variance of activations between layers by scaling using the square root of (2 / (fan_in + fan_out)). (The exact formula varies slightly between the uniform and normal versions.)  
- He Initialization:  
  - Often preferred for layers using ReLU and related functions (such as Leaky ReLU and ELU).  
  - Available as either HeUniform or HeNormal.  
  - It scales weights using the square root of (2 / fan_in).  
- LeCun Initialization:  
  - Particularly useful for networks employing SELU or other self-normalizing techniques.  
  - It scales weights using the square root of (1 / fan_in).

Vanishing vs. Exploding Gradients

Even with appropriate initialization and activation functions, gradients can still:
- Vanish to nearly zero, which slows or stops learning.
- Explode to extremely large values, causing erratic weight updates.

Mitigation Strategies:
- Choose activation functions and weight initialization schemes that help stabilize gradients.
- Batch Normalization: Normalizes the input of each batch to have zero mean and unit variance (with subsequent scaling and shifting). This often speeds up convergence and reduces the risk of vanishing or exploding gradients.
- Gradient Clipping: If gradients exceed a predetermined threshold, they are "clipped" to that threshold, preventing overly large weight updates.