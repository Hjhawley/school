Activation Functions

What is an activation function?  
- In a neuron, an activation function takes the weighted sum of inputs and applies a non-linear transformation (or sometimes linear). This output is then passed to the next layer.  
- The choice of activation function can greatly affect how well gradient descent converges, and whether gradients vanish or explode.

Common Activation Functions

1. Linear (Identity)  
   - Essentially passes values straight through: \(\text{output} = x\).  
   - Smooth and fast, but not typically used in hidden layers if your goal is to learn non-linear patterns; it’s often only used in output layers for regression.

2. ReLU (Rectified Linear Unit)  
   - \(\text{ReLU}(x) = \max(0, x)\).  
   - When \(x\) is negative, output is \(0\); when \(x\) is positive, output is \(x\).  
   - Fast to compute, but not differentiable at \(x=0\); neurons can “die” if they get stuck with negative inputs.

3. Leaky ReLU  
   - \(\text{LeakyReLU}(x) = \max(\alpha x, x)\), where \(\alpha\) is a small constant (e.g., 0.01).  
   - Allows a small, non-zero gradient for negative inputs, preventing “dead” neurons.  
   - Slightly more expensive to compute than ReLU but still relatively fast.

4. ELU (Exponential Linear Unit)  
   - \(\text{ELU}(x) = x\) if \(x > 0\), else \(\alpha (e^x - 1)\).  
   - Smooth and often helps gradient-based learning converge faster. Slightly more expensive to compute than ReLU/Leaky ReLU.  
   - SELU (Scaled ELU) is a variant that includes a scaling constant to normalize outputs automatically, but it’s typically used in special “self-normalizing” networks with only Dense layers and no Batch Normalization.

5. GELU (Gaussian Error Linear Unit)  
   - A smooth function that uses the Gaussian cumulative distribution function.  
   - Can yield good results, but is more computationally expensive than ReLU/Leaky ReLU.

6. SiLU (Swish) / Mish  
   - These are newer smooth activations (SiLU is sometimes called “Swish”):  
     \(\text{silu}(x) = x \cdot \sigma(x)\)  
   - Mish is another smooth function that can sometimes outperform other activations at the cost of higher compute time.

Key Takeaway:  
- Each activation has its pros and cons regarding smoothness, speed, and gradient behavior.  
- ReLU (and variants) remain extremely popular due to simplicity and good performance, but you can experiment if you suspect ReLU is causing too many dead neurons or slow convergence.



Weight Initialization

Proper initialization helps avoid vanishing or exploding gradients.

- RandomUniform: Samples from a uniform distribution. Useful as a general default, but can sometimes lead to suboptimal convergence in deeper networks.  
- RandomNormal: Samples from a Gaussian distribution with specified mean/std.  
- Glorot (Xavier) Initialization:  
  - Often used for sigmoid, tanh, or linear layers.  
  - Can be uniform (`GlorotUniform`) or normal (`GlorotNormal`).  
  - Balances the variance of activations between layers by scaling by \(\sqrt{\frac{2}{(\text{fan\_in} + \text{fan\_out})}}\) (slightly different formula for uniform vs. normal).  
- He Initialization:  
  - Often used for ReLU and related (Leaky ReLU, ELU, etc.).  
  - Can be uniform (`HeUniform`) or normal (`HeNormal`).  
  - Scales by \(\sqrt{\frac{2}{\text{fan\_in}}}\).  
- LeCun Initialization:  
  - Good for SELU or self-normalizing networks.  
  - Scales by \(\sqrt{\frac{1}{\text{fan\_in}}}\).  

> Note: The “pyramid shape” or “no tails” terminology is more of a conceptual or visual way of describing how these initializers constrain the variance. In practice, they each have specific mathematical formulas to preserve variance and avoid overly large or tiny gradients.



Vanishing vs. Exploding Gradients

Even with good initializers and activation functions, gradients can still:  
- Vanish to near zero, slowing or halting learning.  
- Explode to very large values, causing erratic updates.

Mitigation Strategies  
- Use appropriate activation functions and weight initialization schemes.  
- Batch Normalization: Applies normalization (zero mean, unit variance, then scale/shift) at each batch, which often speeds convergence and can reduce the risk of vanishing/exploding gradients.  
- Gradient Clipping: If the gradient exceeds a threshold, it’s “clipped” to that threshold. This prevents excessively large weight updates.



Optimizers
Gradient Descent & Convergence

- Goal: Stop quickly (fewer epochs) and converge to a good minimum.  
- Momentum: Speeds up training by incorporating a fraction of the previous update’s velocity.  
  ```python
  keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)
  ```
  - Nesterov . . .