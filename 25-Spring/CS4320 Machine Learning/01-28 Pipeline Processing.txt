Pipeline Processing
Get data, prepare data, fit data → we want to integrate this into an easily repeatable process.

We have a scaler that we fit to the data; 
then transform the data; then send it to a model; 
and finally, tell the model to fit the data.
This is a pipeline process.

Whenever we need to predict new data, it must first be transformed.
We want to bake this entire process into one step and abstract it away.
This works because every element in the pipeline has both a .fit() and a .transform() method.

Example pipeline: 
    [PP1 → PP2 → Model]

Calling .fit(x, y) on this pipeline automates the following processes:
    1. Pre-processing Element 1:
        .fit(x)
        .transform(x) → Produces x_1
    2. Pre-processing Element 2:
        .fit(x_1)
        .transform(x_1) → Produces x_2
    3. Model:
        .fit(x_2, y) → Trains the model.

After this, we can call .predict(z), which:
    Transforms the input z (but does not fit intermediate steps).
    Produces predicted labels y^.

* * * * * * * * * * * * * * * * * * * *

But what if the form of our function is not good enough?
For example, what if we're wasting time looking for coefficients in a single-order polynomial 
when the actual relationship is quadratic?

To address this, we need to ENRICH the hypothesis space by allowing for more complex data structures.
We still say y is some function of x, but we expand the feature space to include additional terms.
We can square or cube features, cross-product different features ("interaction terms"),
take the log or exponent of features, apply trigonometric functions, and more.

Example:
With three features x1 x2 x3, instead of the function:
    y = θ_0 + θx1 + θx2 + θx3
We expand it to something like:
    y = θ_0 + θx1 + θx2 + θx3 + θx1^2 + θx2^2 + θx3^2 + θx1x2 + θx2x3 + θx1x3

The coefficients θ are still linear, but the features are represented in a more complex space.
This process helps identify which dependencies are important and which are not.